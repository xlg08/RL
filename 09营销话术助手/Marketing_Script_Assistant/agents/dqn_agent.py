"""
æœ¬é¡¹ç›®æ˜¯RLHFä»»åŠ¡ï¼Œé€šè¿‡äººå·¥åé¦ˆè¿›è¡Œå­¦ä¹ ã€‚ä¸ºäº†æ›´æ¥è¿‘å®é™…ä½¿ç”¨åœºæ™¯ï¼Œé€‰æ‹©off-policyæ¥è¿›è¡Œè®­ç»ƒã€‚
ç®—æ³•å±‚é¢ï¼Œé€‰ç”¨DQNç®—æ³•ï¼Œ å®ƒæ˜¯ä¸€ç§off-policyç®—æ³•ï¼Œä½¿ç”¨ replay buffer å­˜å‚¨å†å²ç»éªŒï¼Œ
æ”¯æŒæ‰‹åŠ¨æ‰©å±• replay bufferï¼Œé€šè¿‡ replay_buffer.add()æ”¯æŒå¤–éƒ¨æ•°æ®ï¼Œ
"""

# from agents.offline_dqn import OfflineDQN
from offline_dqn import OfflineDQN
from stable_baselines3.common.buffers import ReplayBuffer
import os
import numpy as np

project_root = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
MODEL_PATH = os.path.join(project_root, "models", "dqn_marketing_model")


class DQNAgent:
    def __init__(self, env):
        self.env = env
        if os.path.exists(f"{MODEL_PATH}.zip"):
            self.model = OfflineDQN.load(MODEL_PATH)
            print("âœ… åŠ è½½å·²æœ‰æ¨¡å‹")
        else:
            self.model = OfflineDQN("MlpPolicy",
                                    self.env,
                                    verbose=1,
                                    buffer_size=100000,
                                    exploration_fraction=0.3,  # å¢åŠ æ¢ç´¢æ—¶é—´æ¯”ä¾‹
                                    exploration_initial_eps=1.0,  # åˆå§‹æ¢ç´¢ç‡
                                    exploration_final_eps=0.01,  # æœ€ç»ˆæ¢ç´¢ç‡ï¼ˆæ›´å°çš„å€¼ï¼‰
                                    learning_starts=1000,  # å¢åŠ å¼€å§‹å­¦ä¹ å‰çš„æ­¥æ•°
                                    target_update_interval=500  # æ›´æ–°ç›®æ ‡ç½‘ç»œçš„é¢‘ç‡
                                    )
            self.model.exploration_rate = 1.0
            print("ä½¿ç”¨æ–°æ¨¡å‹åˆå§‹åŒ–ï¼Œå¹¶å¼ºåˆ¶å¼€å¯ 100% éšæœºæ¢ç´¢")
            print("ğŸ†• ä½¿ç”¨æ–°æ¨¡å‹åˆå§‹åŒ–")

    def train(self, total_timesteps, dataset=None):
        if dataset is not None:
            # # å°† dataset è½¬æ¢ä¸º replay buffer æ”¯æŒçš„æ ¼å¼
            # observations = []
            # actions = []
            # rewards = []
            # next_observations = []
            # dones = []

            buffer_size = len(dataset)
            self.model.replay_buffer = ReplayBuffer(
                buffer_size,
                self.model.observation_space,
                self.model.action_space,
                device=self.model.device,
                n_envs=1
            )

            for item in dataset:
                # æ¯æ¬¡å¤„ç†ä¸€æ¡ transition
                obs = np.array(item["state"], dtype=np.float32)
                next_obs = np.array(item["next_state"], dtype=np.float32)
                action = np.array([item["action"]], dtype=np.int8)  # shape: (1,)
                reward = np.array(item["reward"], dtype=np.float32)
                done = np.array(item["done"], dtype=bool)

                # âœ… ä¸€æ¡ä¸€æ¡åœ°æ·»åŠ è¿› buffer
                self.model.replay_buffer.add(
                    obs=obs,
                    next_obs=next_obs,
                    action=action,
                    reward=reward,
                    done=done,
                    infos=[{}]
                )
            print(f"âœ… æˆåŠŸå‘ replay buffer æ·»åŠ  {len(dataset)} æ¡æ•°æ®")

        # å¼€å§‹è®­ç»ƒ
        self.model.learn(total_timesteps=total_timesteps)

    def predict(self, observation):
        # deterministic=False è¡¨ç¤ºå…è®¸æ¢ç´¢
        action, _states = self.model.predict(observation, deterministic=False)
        return action.item()

    def save(self):
        self.model.save(MODEL_PATH)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_PATH}.zip")
if __name__ == '__main__':
    from environment.dialogue_env import MarketingDialogueEnv
    env = MarketingDialogueEnv()
    # åˆ›å»ºDQNæ™ºèƒ½ä½“å®ä¾‹
    agent = DQNAgent(env)
    action = agent.predict(np.zeros(384))
    print(action)

